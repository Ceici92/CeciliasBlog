<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Reinforcement Learning : Part 2 | Cecilia&#39;s Blog</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.75.1" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="https://Ceici92.github.io/CeciliasBlog/dist/css/app.4fc0b62e4b82c997bb0041217cd6b979.css" rel="stylesheet">
    

    

    
      

    

    
    
    <meta property="og:title" content="Reinforcement Learning : Part 2" />
<meta property="og:description" content="This post is the second part of my work for the Reinforcement Learning labaratory. I presented the reinforcement learning model in the previous post, and here I am going to explain what I changed in the model to see the impact of the different parameters.
Part 2 : Manipulations First test I conducted a first training with the basic following parameters :
And I looked at the first results without changing any of those parameters :" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Ceici92.github.io/CeciliasBlog/posts/rl_2/" />
<meta property="article:published_time" content="2020-12-15T18:14:31+01:00" />
<meta property="article:modified_time" content="2020-12-15T18:14:31+01:00" />
<meta itemprop="name" content="Reinforcement Learning : Part 2">
<meta itemprop="description" content="This post is the second part of my work for the Reinforcement Learning labaratory. I presented the reinforcement learning model in the previous post, and here I am going to explain what I changed in the model to see the impact of the different parameters.
Part 2 : Manipulations First test I conducted a first training with the basic following parameters :
And I looked at the first results without changing any of those parameters :">
<meta itemprop="datePublished" content="2020-12-15T18:14:31+01:00" />
<meta itemprop="dateModified" content="2020-12-15T18:14:31+01:00" />
<meta itemprop="wordCount" content="663">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Reinforcement Learning : Part 2"/>
<meta name="twitter:description" content="This post is the second part of my work for the Reinforcement Learning labaratory. I presented the reinforcement learning model in the previous post, and here I am going to explain what I changed in the model to see the impact of the different parameters.
Part 2 : Manipulations First test I conducted a first training with the basic following parameters :
And I looked at the first results without changing any of those parameters :"/>

	
  </head>

  <body class="ma0 avenir bg-light-yellow">

    
   
  

  
  
  <header class="cover bg-top" style="background-image: url('https://github.com/Ceici92/RL_1/VideoHammer.gif?raw=true');">
    <div class="pb3-m pb6-l bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://Ceici92.github.io/CeciliasBlog/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Cecilia&#39;s Blog
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://Ceici92.github.io/CeciliasBlog/posts/" title="Posts page">
              Posts
            </a>
          </li>
          
        </ul>
      
      















    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          <h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Reinforcement Learning : Part 2</h1>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        POSTS
      </aside>
      




  <div id="sharing" class="mt3">

    
    <a href="https://www.facebook.com/sharer.php?u=https://Ceici92.github.io/CeciliasBlog/posts/rl_2/" class="facebook no-underline" aria-label="share on Facebook">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

    </a>

    
    
    <a href="https://twitter.com/share?url=https://Ceici92.github.io/CeciliasBlog/posts/rl_2/&amp;text=Reinforcement%20Learning%20:%20Part%202" class="twitter no-underline" aria-label="share on Twitter">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

    </a>

    
    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://Ceici92.github.io/CeciliasBlog/posts/rl_2/&amp;title=Reinforcement%20Learning%20:%20Part%202" class="linkedin no-underline" aria-label="share on LinkedIn">
      <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

    </a>
  </div>


      <h1 class="f1 athelas mt3 mb1">Reinforcement Learning : Part 2</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2020-12-15T18:14:31+01:00">December 15, 2020</time>

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><p>This post is the second part of my work for the Reinforcement Learning labaratory.
I presented the reinforcement learning model in the previous post, and here I am going to explain what I changed in the model to see the impact of the different parameters.</p>
<h1 id="part-2--manipulations">Part 2 : Manipulations</h1>
<h2 id="first-test">First test</h2>
<p>I conducted a first training with the basic following parameters :</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/FirstTest.png?raw=true" alt="alt Text" title="First Test"></p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/FirstTest2.png?raw=true" alt="alt Text" title="First Test 2"></p>
<p>And I looked at the first results without changing any of those parameters :</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/FirstTest3.png?raw=true" alt="alt Text" title="First Test 3"></p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/FirstTest3.png?raw=true" alt="alt Text" title="First Test 4"></p>
<p>The loss varies a lot, because the training dataset is changing over time. If we look at the score, it seems stable around 12.</p>
<h2 id="the-training-algorithm">The Training Algorithm</h2>
<p>To observe the impact of the training algorithm on the results, I changed it to Sarsa and conducted another evaluation :</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/Sarsa.png?raw=true" alt="alt Text" title="Sarsa">
<img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/Sarsa2.png?raw=true" alt="alt Text" title="Sarsa 2"></p>
<p>The new Sarsa algorithm did not impact a lot the resulting score of the system, it stays around 12.
So I went back to QLearning and I tried changing other parameters.</p>
<h2 id="the-doer">The Doer</h2>
<p>I changed the Doer to randomly to look at the impact on the results.
The score becomes quite hight (around 23), however we observe that it is more stable because in this case the model does not learn :</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/Doer.png?raw=true" alt="alt Text" title="Doer"></p>
<p>I turned it back to the previous Doer to keep observing the influence of the other parameters.</p>
<h2 id="the-experience-replay">The Experience Replay</h2>
<p>To obtain better scores, I first changed the width of the network.
With a capacity of 16 the network was not going to be able to learn enough to reach higher scores.
Therefore, I augmented the width of the network to 128. Besides, I diminished the gamma to 0.90, added more Experience Replays, and augmented the buffer size in order to let the system remember more transitions at the same time :</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/ER.png?raw=true" alt="alt Text" title="ERBis">
<img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/ER.png?raw=true" alt="alt Text" title="ER"></p>
<p>The score is much higher, it almost reaches 40 and then varies around 20. However, even if at the beginning the model seem to learn and progress, its score starts to decrease around the 300th iteration. Our model reaches its second wall.</p>
<p>The loss is also smaller :
<img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/ER2.png?raw=true" alt="alt Text" title="ER2">
<img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/ER3.png?raw=true" alt="alt Text" title="ER3"></p>
<h2 id="the-reward-function">The Reward Function</h2>
<p>I augmented the batch size of the Experience Replays to try to improve the results.
Then, I changed the reward function as following :
<img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/Reward.png?raw=true" alt="alt Text" title="Reward"></p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/Reward2.png?raw=true" alt="alt Text" title="Reward2"></p>
<p>The system starts with low scores and improves until reaching a score around 50, before fluctuating a lot, and going down to a score of 20.</p>
<p>I conducted another evaluation with exactly the same parameters.
Once again, we can observe that the system improves until a hight score (81) and then goes down to a score of 15.
<img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/Reward3.png?raw=true" alt="alt Text" title="Reward3"></p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/Reward4.png?raw=true" alt="alt Text" title="Reward4"></p>
<p>Besides, we observe that some episodes last more than 100 iterations since the score does not change, as you can see on the highlighted scores above.
The model has reached another wall.
However we also observe huge decreases of the reward and the score : it comes from the fact that the model enters a new area of environment and it forgets how to behave at the beginning of a episode.</p>
<h2 id="the-dataset">The Dataset</h2>
<p>The manipulation of the training dataset is also what enables the system to reach such high scores, it was already set to only train the system every 3 time steps :</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/Dataset.png?raw=true" alt="alt Text" title="Dataset"></p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/Dataset2.png?raw=true" alt="alt Text" title="Dataset"></p>
<p>On top, we can see the results with the same parameters but with basic utilisation of the dataset.
The system does not success in reaching a higher score than 30, because it does not have time to gather enough data.
This is why training the model only each 3 iterations, enables the system to gather more data before training, and therefore to improve its choices.</p>
<h2 id="the-activation-function">The Activation Function</h2>
<p>Finally, I changed the activation function from Sigmoid to LeakyRelu, to see the impact on the system :</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/CeciliasBlog/blob/master/docs/images/RL_2/ActivationF.png?raw=true" alt="alt Text" title="ActivationF"></p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/ActivationF2.png?raw=true" alt="alt Text" title="ActivationF2">
<img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/ActivationF3.png?raw=true" alt="alt Text" title="ActivationF3"></p>
<p><a href="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/ActivationF.png">https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_2/ActivationF.png</a></p>
<p>The results are not better, it is the contrary : the score fluctuates from 25 to 15.
Therefore the Sigmoid function seems more suited for this model.</p>
<p>We could also change the value of the epsilon to observe the influence of the amount of exploration on the performances of the system.</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-dark-gray bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://Ceici92.github.io/CeciliasBlog" >
    &copy;  Cecilia's Blog 2020 
  </a>
    <div>














</div>
  </div>
</footer>

    

  <script src="https://Ceici92.github.io/CeciliasBlog/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
