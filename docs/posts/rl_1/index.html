<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Reinforcement Learning : Part 1 | Cecilia&#39;s Blog</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.75.1" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="https://Ceici92.github.io/CeciliasBlog/dist/css/app.4fc0b62e4b82c997bb0041217cd6b979.css" rel="stylesheet">
    

    

    
      

    

    
    
    <meta property="og:title" content="Reinforcement Learning : Part 1" />
<meta property="og:description" content="In this post I will explain one of the laboratories I did this year about Reinforcement Learning. During this lab, we used a python code on Google Collab to manipulate a deep reinforcement learning network to solve the CartPole game : a game where a hammer must not fall.
In this first part, I am going to explain the different functions of the model, and in the second part in the next post, I will explain what I changed in the model to see the impact of the different parameters." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Ceici92.github.io/CeciliasBlog/posts/rl_1/" />
<meta property="article:published_time" content="2020-12-15T14:59:24+01:00" />
<meta property="article:modified_time" content="2020-12-15T14:59:24+01:00" />
<meta itemprop="name" content="Reinforcement Learning : Part 1">
<meta itemprop="description" content="In this post I will explain one of the laboratories I did this year about Reinforcement Learning. During this lab, we used a python code on Google Collab to manipulate a deep reinforcement learning network to solve the CartPole game : a game where a hammer must not fall.
In this first part, I am going to explain the different functions of the model, and in the second part in the next post, I will explain what I changed in the model to see the impact of the different parameters.">
<meta itemprop="datePublished" content="2020-12-15T14:59:24+01:00" />
<meta itemprop="dateModified" content="2020-12-15T14:59:24+01:00" />
<meta itemprop="wordCount" content="1169">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Reinforcement Learning : Part 1"/>
<meta name="twitter:description" content="In this post I will explain one of the laboratories I did this year about Reinforcement Learning. During this lab, we used a python code on Google Collab to manipulate a deep reinforcement learning network to solve the CartPole game : a game where a hammer must not fall.
In this first part, I am going to explain the different functions of the model, and in the second part in the next post, I will explain what I changed in the model to see the impact of the different parameters."/>

	
  </head>

  <body class="ma0 avenir bg-light-yellow">

    
   
  

  
  
  <header class="cover bg-top" style="background-image: url('https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/Part1.png?raw=true?raw=true');">
    <div class="pb3-m pb6-l bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://Ceici92.github.io/CeciliasBlog/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Cecilia&#39;s Blog
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://Ceici92.github.io/CeciliasBlog/posts/" title="Posts page">
              Posts
            </a>
          </li>
          
        </ul>
      
      















    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          <h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Reinforcement Learning : Part 1</h1>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        POSTS
      </aside>
      




  <div id="sharing" class="mt3">

    
    <a href="https://www.facebook.com/sharer.php?u=https://Ceici92.github.io/CeciliasBlog/posts/rl_1/" class="facebook no-underline" aria-label="share on Facebook">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

    </a>

    
    
    <a href="https://twitter.com/share?url=https://Ceici92.github.io/CeciliasBlog/posts/rl_1/&amp;text=Reinforcement%20Learning%20:%20Part%201" class="twitter no-underline" aria-label="share on Twitter">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

    </a>

    
    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://Ceici92.github.io/CeciliasBlog/posts/rl_1/&amp;title=Reinforcement%20Learning%20:%20Part%201" class="linkedin no-underline" aria-label="share on LinkedIn">
      <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

    </a>
  </div>


      <h1 class="f1 athelas mt3 mb1">Reinforcement Learning : Part 1</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2020-12-15T14:59:24+01:00">December 15, 2020</time>

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><p>In this post I will explain one of the laboratories I did this year about Reinforcement Learning.
During this lab, we used a python code on Google Collab
to manipulate a deep reinforcement learning network to solve the CartPole game :
a game where a hammer must not fall.</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/Hammer.JPG?raw=true" alt="alt Text" title="Hammer Game"></p>
<p>In this first part, I am going to explain the different functions of the model, and in the second part in the next post, I will explain what I changed in the model to see the impact of the different parameters.</p>
<h1 id="part-1--presentation-of-the-model">Part 1 : Presentation of the model</h1>
<p>First, we have the imports of the different objects needed to create our model. For example, gym is for the simulation of the Cart Pole game, and numpy is for the big amount of data used.</p>
<h2 id="the-qnetwork">The QNetwork</h2>
<p>The second Class creates the QNetwork used to implement the Qvalues of the model :</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/QNetwork.png?raw=true" alt="alt Text" title="QNetwork"></p>
<p>We can already identify some parameters we will be able to manipulate : the width of the network (here at 64), the activation function set at sigmoid, and the number of hidden layers.</p>
<p>As in a supervised network, the dataset given will be taken as an input of the network, it will pass through the different layers of the model (the hidden layers are all the layers that are not the input and output ones).
However, contrarily to a supervised network, the output of the layers is the score of each action of the agent at each state.</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/QNetwork2.png?raw=true" alt="alt Text" title="QNetwork 2"></p>
<p>Since the basic Qnetwork only takes as an input the states, the python code returns a pair of a state and its given action as needed.</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/QNetwork3.png?raw=true" alt="alt Text" title="QNetwork 3"></p>
<p>The Qfunction returns the Qvalue for the given state in input.</p>
<p>Then, the model is printed to verify that there is no problem, along with it results for 10 random states from the gym dataset :</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/QNetworkTest.png?raw=true" alt="alt Text" title="QNetwork test"></p>
<p>We observe that the results are given in a matrix with two columns, each represents the qvalue of an action, and as we can see the action 1 is often the best because its qvalues are higher.</p>
<h2 id="the-doer">The Doer</h2>
<p>The Doer is the part of the agent that acts : it takes the qnetwork, evaluates it on an input state and decides which action the agent should keep in function of the output qvalues of the network.</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/Doer.png?raw=true" alt="alt Text" title="Doer"></p>
<p>One of its parameters is the epsilon set at 10%, it indicates the amount of time where the Doer behaves randomly.
It designates the amount of exploration the model will do.</p>
<p>Then we test the Doer, and see that it choses the accurate action with the higher qvalue :
<img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/DoerTest.png?raw=true" alt="alt Text" title="Doer test"></p>
<h2 id="the-experience-replay">The Experience Replay</h2>
<p>This function encapsulates all the relevant data from the agent transitions : St (the previous state), At (the previous action), Rt (the reward), St+1 (the current state), and At+1 (the current action).</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/ExpReplay.png?raw=true" alt="alt Text" title="Experience Replay"></p>
<p>This function encapsulates all the relevant data from the agent transitions : St (the previous state), At (the previous action), Rt (the reward), St+1 (the current state), and At+1 (the current action).
With the variable ID, the class identifies each transition, in order in the future to assign them a relevance correlated to their loss.
Each transition also has a birthdate, to keep in mind their order.</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/ExpReplay2.png?raw=true" alt="alt Text" title="Experience Replay 2"></p>
<p>The ExperienceReplay class encapsulates the list of transitions in an instance.
One of the class parameters is the buffer size, that defines the number of transition that the system can hold at every time step.</p>
<p>The two following parameters designate how the system will choose which transition it is going to keep.
The weightBatches defines if we choose the more relevant transition (ie. the one that submits the less loss) or if we choose them uniformly (False).
The SortTransition defines, when the batch is full, if the system removes the older transitions (False) or the one less useful (True).</p>
<p>In the sampleBatch method the system designates which transitions in the batch it will choose to train the network, in function of the parameters chosen previously.</p>
<p>To see how the class works, we print all the relevance of the transitions in an ER, and the ones kept in a batch :
<img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/ExpReplayTest.png?raw=true" alt="alt Text" title="Experience Replay test"></p>
<h2 id="the-learner">The Learner</h2>
<p>This class trains a qnetwork with the batches from the Experience Replay class.</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/Learner.png?raw=true" alt="alt Text" title="Learner"></p>
<p>If we look at the parameters : gamma is the discounting factor, and the algorithm used to improve the network is either Sarsa or QLearning.</p>
<p>We observe that there is a target network that will be trained, and a frozen network that will be used to see the improvement of the network along the training.</p>
<p>Another variable in this class is the optimizer that is set to adam, it is another parameter that could be changed in the next part to observe its impact on the system.</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/Learner2.png?raw=true" alt="alt Text" title="Learner 2"></p>
<p>The learn method updates the equations of the algorithm chosen to improve the system.
You can see circled in red the targeted qvalues, and at their left the current qvalues.
The subtraction of those two defines the error of the system, which is used to define the loss of each transition.</p>
<p>The Sarsa algorithm looks at what the system did after (At+1) to define the target qvalues, so it is ON Policy, while the Q-Learning algorithm looks at the maximum, it is OFF Policy.</p>
<p>Then the system gets the batches and makes sure there is no aberration in the data. It extracts all the objects from the batches, and set the target network to train :
<img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/Learner3.png?raw=true" alt="alt Text" title="Learner 3"></p>
<p>For every transition, step, iteration, in the batch, the qvalue becomes closer to the target qvalue of the algorithm.</p>
<p>After the training, the results are displayed to observe the evolution of the batch :</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/LearnerTest.png?raw=true" alt="alt Text" title="Learner test">
<img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/LearnerTest2.png?raw=true" alt="alt Text" title="Learner test 2"></p>
<p>The loss is supposed to be smaller after the training, and circled in red we can observe the parameters of the batch at the beginning of the training.</p>
<h2 id="the-training-loop">The Training Loop</h2>
<p>The 7th class is the Training Loop.
The model has been defined, and now it is time to train it to sruvive episodes of the game.</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/TrainingLoop.png?raw=true" alt="alt Text" title="Training Loop"></p>
<p>The initialization of this class will be the core of our playground during the next part of this article, because there is a lot of parameters to change.
The initialization concerns all the parameters of the model that will act during the training : the qnetwork, the doer, the learner, and the ERs list.</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/TrainingLoop2.png?raw=true" alt="alt Text" title="Training Loop 2"></p>
<p>At every iteration, the agent acts, the system adds a transition in the Experience Replay, this replay is used by the Learner class to improve the model, and the system updates the batch with better actions for the agent.</p>
<p>At each iteration we print the loss resulting from the action and the transition of the agent, along with the score in the game, and the epsilon :</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/TrainingLoop3.png?raw=true" alt="alt Text" title="Training Loop test"></p>
<h2 id="the-evaluation">The Evaluation</h2>
<p>The 8th class is the Evaluation : it displays in a graphic the resulting score of the training of the system :</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/Evaluation.png?raw=true" alt="alt Text" title="Evaluation"></p>
<h2 id="the-renderer">The Renderer</h2>
<p>Finally, the last class renders the movement of the agent during the episode, I did not run it because it took a lot of time.</p>
<p><img src="https://github.com/Ceici92/CeciliasBlog/blob/master/docs/images/RL_1/Hammer.JPG?raw=true" alt="alt Text" title="Renderer"></p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-dark-gray bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://Ceici92.github.io/CeciliasBlog" >
    &copy;  Cecilia's Blog 2020 
  </a>
    <div>














</div>
  </div>
</footer>

    

  <script src="https://Ceici92.github.io/CeciliasBlog/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
